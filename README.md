# Chatbot with RAG and Gemma3

This project implements a Retrieval-Augmented Generation (RAG) chatbot using:
- **Chroma** as the vector database
- **nomic-embed-text** from Ollama for embeddings
- **Gemma3 LLM** for answering questions

The chatbot has two main APIs:
1. **Upload API** â€“ Upload documents, chunk them, embed with nomic-embed-text, and store in Chroma DB.
2. **Chat API** â€“ Query the chatbot; relevant chunks are retrieved from Chroma and passed to Gemma3 for generating answers.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app.py                # FastAPI entry point (defines APIs)
â”œâ”€â”€ config.py             # Configuration settings
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ chunker.py        # Splits documents into chunks
â”‚   â”œâ”€â”€ loader.py         # Loads documents from /data
â”‚   â”œâ”€â”€ utils.py          # Helper utilities
â”‚   â””â”€â”€ vector_store.py   # Handles Chroma DB operations
â”œâ”€â”€ data/                 # Uploaded documents storage
â””â”€â”€ chroma_db/            # Persistent Chroma vector database
```

---

## âš™ï¸ Setup

### 1. Clone repository & create venv
```bash
git clone https://github.com/your-repo/chatbot-rag.git
cd chatbot-rag
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
.venv\Scripts\activate    # Windows
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Run Ollama (for embeddings & LLM)
Ensure you have [Ollama](https://ollama.ai) installed and running.

Pull required models:
```bash
ollama pull nomic-embed-text
ollama pull gemma3
```

### 4. Run the FastAPI app
```bash
uvicorn app:app --reload --host 127.0.0.1 --port 8000
```

---

## ğŸš€ API Endpoints

### 1. Upload Document
**POST** `/upload`  
Uploads a document, chunks it, embeds with `nomic-embed-text`, and stores in Chroma DB.

Request (multipart/form-data):
```
file=@example.pdf
```

Response:
```json
{"message": "File uploaded and indexed successfully."}
```

---

### 2. Chat with Bot
**POST** `/chat`  
Ask a question, retrieve knowledge from Chroma, and generate an answer with `gemma3`.

Request:
```json
{
  "query": "How I can use this website?"
}
```

Response:
```json
{
  "answer": "This website is for..."
}
```

---

## ğŸ“Œ Notes
- Embeddings are stored persistently inside `chroma_db/`.
- Uploaded documents remain in `data/`.
- Multi-language support depends on LLM capability (`gemma3` supports multiple languages).

---

## ğŸ›  Future Improvements
- Add authentication for APIs  
- Improve chunking strategy  
- Support streaming responses  
- Add language detection + translation in RAG pipeline  

---

## ğŸ§‘â€ğŸ’» Author
Developed by L0CH1NBEK
